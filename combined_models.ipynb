{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc1f021c-5d36-4963-a41d-c7f86d43929a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "007eff28-d1f2-4b84-9d71-37bf1df892f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords, brown\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de667a9b-839b-4a34-9e89-77a15630fbee",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81f4f7ce-123b-4216-9f99-75696931b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_name = 'Magazine_Subscriptions'\n",
    "min_amount_product_mentions = 4\n",
    "min_amount_user_mentions = 4\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "MIN_WORDS = 4\n",
    "MAX_WORDS = 200\n",
    "\n",
    "PATTERN_S = re.compile(\"\\'s\")  # matches `'s` from text  \n",
    "PATTERN_RN = re.compile(\"\\\\r\\\\n\") #matches `\\r` and `\\n`\n",
    "PATTERN_PUNC = re.compile(r\"[^\\w\\s]\") # matches all non 0-9 A-z whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94841b58-0dbd-401e-ae36-14e9316ade36",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "403df00a-550e-4fb8-a01e-4b42050aa9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_metadata(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer() \n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Series of cleaning. String to lower case, remove non words characters and numbers.\n",
    "        text (str): input text\n",
    "    return (str): modified initial text\n",
    "    \"\"\"\n",
    "    text = text.lower()  # lowercase text\n",
    "    text = re.sub(PATTERN_S, ' ', text)\n",
    "    text = re.sub(PATTERN_RN, ' ', text)\n",
    "    text = re.sub(PATTERN_PUNC, ' ', text)\n",
    "    return text\n",
    "\n",
    "def tokenizer(sentence, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS, lemmatize=True):\n",
    "    \"\"\"\n",
    "    Lemmatize, tokenize, crop and remove stop words.\n",
    "    \"\"\"\n",
    "    if lemmatize:\n",
    "        stemmer = WordNetLemmatizer()\n",
    "        tokens = [stemmer.lemmatize(w) for w in nltk.word_tokenize(sentence)]\n",
    "    else:\n",
    "        tokens = [w for w in word_tokenize(sentence)]\n",
    "    token = [w for w in tokens if (len(w) > min_words and len(w) < max_words\n",
    "                                                        and w not in stopwords)]\n",
    "    return tokens    \n",
    "\n",
    "\n",
    "def clean_sentences(df):\n",
    "    \"\"\"\n",
    "    Remove irrelavant characters (in new column clean_sentence).\n",
    "    Lemmatize, tokenize words into list of words (in new column tok_lem_sentence).\n",
    "    \"\"\"\n",
    "    print('Cleaning sentences...')\n",
    "    df['clean_sentence'] = df['sentence'].apply(clean_text)\n",
    "    df['tok_lem_sentence'] = df['clean_sentence'].apply(\n",
    "        lambda x: tokenizer(x, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS, lemmatize=True))\n",
    "    return df\n",
    "\n",
    "def get_data(subset_name, min_amount_product_mentions, min_amount_user_mentions, test_size=0.1, random_state=42):    \n",
    "    data = []\n",
    "    with gzip.open(os.path.join('data', f'meta_{subset_name}.json.gz')) as f:\n",
    "        for l in f:\n",
    "            data.append(json.loads(l.strip()))\n",
    "    meta_df = pd.DataFrame.from_dict(data)\n",
    "    \n",
    "    meta_df['category'] = meta_df['category'].apply(lambda x: ','.join(map(str, x)))\n",
    "    meta_df['description'] = meta_df['description'].apply(lambda x: ','.join(map(str, x)))\n",
    "    meta_df['feature'] = meta_df['feature'].apply(lambda x: ','.join(map(str, x)))\n",
    "    meta_df['metadata'] = meta_df['category'] + ' ' + meta_df['description'] + ' ' + meta_df['title'] + ' ' + meta_df['feature']\n",
    "    \n",
    "    meta_df['metadata'] = meta_df['metadata'].map(lambda x:preprocess_metadata(x)) \n",
    "    \n",
    "    data = []\n",
    "    with gzip.open(os.path.join('data', f'{subset_name}.json.gz')) as f:\n",
    "        for l in f:\n",
    "            data.append(json.loads(l.strip()))\n",
    "    rating_df = pd.DataFrame.from_dict(data)\n",
    "    rating_df = rating_df[['asin', 'reviewerID', 'overall', 'reviewText']]\n",
    "    \n",
    "    rating_df = rating_df.drop_duplicates()    \n",
    "    rating_df = rating_df[rating_df['asin'].map(rating_df['asin'].value_counts()) >= min_amount_product_mentions]\n",
    "    rating_df = rating_df[rating_df['reviewerID'].map(rating_df['reviewerID'].value_counts()) >= min_amount_user_mentions]\n",
    "    rating_df = rating_df[~rating_df.reviewText.isna()]\n",
    "    rating_df.rename(columns={'reviewText':'sentence'}, inplace=True)\n",
    "    rating_df = clean_sentences(rating_df)\n",
    "    \n",
    "    rating_df = pd.merge(rating_df, meta_df[['asin', 'metadata']], on='asin')\n",
    "    \n",
    "    # split rating df in half -> half is used for sentiment analysis and the other half for the recommender models\n",
    "    X = rating_df.copy().drop(['overall'], axis=1)\n",
    "    y = rating_df['overall']\n",
    "    \n",
    "    X_rm, X_sa, y_rm, y_sa = train_test_split(X, y, test_size=0.5, stratify=y, random_state=random_state)\n",
    "        \n",
    "    # split data of the sa half for the test set\n",
    "    X_sa, X_test, y_sa, y_test = train_test_split(X_sa, y_sa, test_size=test_size, stratify=y_sa, random_state=random_state)\n",
    "                        \n",
    "    return rating_df, X_rm, X_sa, X_test, y_rm, y_sa, y_test, meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62898617-48e6-410e-943d-4ef666791457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning sentences...\n"
     ]
    }
   ],
   "source": [
    " rating_df, X_rm, X_sa, X_test, y_rm, y_sa, y_test, meta_df = get_data(subset_name, min_amount_product_mentions, min_amount_user_mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ccc0f6-209c-463a-bc01-26c5567ca2d0",
   "metadata": {},
   "source": [
    "# Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6062ebb3-7d82-47b9-a620-0eeb3e08fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    print(model.__class__.__name__)\n",
    "    print(f'RMSE: {np.sqrt(mean_squared_error(y, y_pred))}')\n",
    "    print(f'MAE: {mean_absolute_error(y, y_pred)}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dc02f6f-ccd5-4fd9-aebf-b3036ace2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_product_ids():\n",
    "    return rating_df['asin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf04e5ba-3a74-484e-b16d-addc74fdb95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_recommendations(model, user_id, k=5):    \n",
    "    # only recommend items not reviewed by the user before\n",
    "    not_rated_product_ids = [x for x in get_all_product_ids() if x not in set(rating_df[rating_df['reviewerID'] == 'AEJAGHLC675A7']['asin'].values)]\n",
    "        \n",
    "    user_product_pairs = pd.DataFrame({\n",
    "        'asin': not_rated_product_ids,\n",
    "        'reviewerID': [user_id]*len(not_rated_product_ids)\n",
    "    })\n",
    "    user_product_pairs = pd.merge(user_product_pairs, meta_df[['asin', 'metadata']], on='asin')\n",
    "    user_product_pairs['predicted_rating'] = model.predict(user_product_pairs)\n",
    "    return user_product_pairs.sort_values(by='predicted_rating', ascending=False).head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b6f33c1-ad70-484e-8c82-76c52459ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_k_recommendations(model, user_id, k=5):\n",
    "    print(f'User {user_id} has previously enjoyed:')\n",
    "    previously_rated = rating_df[rating_df['reviewerID']==user_id].sort_values(by='overall',ascending=False).head(k)['asin'].values\n",
    "    print_product_titles(previously_rated)\n",
    "    print('')\n",
    "    print('We now recommend him:')\n",
    "    recommendations =  get_k_recommendations(model, user_id, k)['asin'].values\n",
    "    print_product_titles(recommendations)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "64a3119b-5003-49dd-817d-e8ffa61250c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_product_titles(ids):\n",
    "    titles = meta_df[meta_df['asin'].isin(ids)]['title'].values\n",
    "    for title in titles:\n",
    "        print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564f94a-82ca-4579-9f57-2211fcc012b5",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1725a8b-f7c5-4cab-a3d3-195d403548b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf54fbf9-cc27-474b-810f-0eda53579d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    \n",
    "    def __init__(self, X_train, y_train):\n",
    "        self._X_train = X_train.copy()\n",
    "        self._y_train = y_train.copy()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return [2.5] * len(X)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c56a3-52f5-4094-855f-2e0801334635",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Content-based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c2c51ca-8699-4a24-b14c-974a3f256fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBasedModel(BaseModel):\n",
    "    \n",
    "    def __init__(self, X_train, y_train, meta_df):\n",
    "        self._pipeline = Pipeline([('Vectorizer', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('SVM classifier', SVC())])\n",
    "\n",
    "        self._pipeline.fit(X_train['metadata'], y_train)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self._pipeline.predict(X['metadata'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb8b07e-fd96-4ef7-b24e-75ed09571e93",
   "metadata": {},
   "source": [
    "## Collab Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d2b9ede6-5a82-404b-a344-ac0d5831718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollabWeightedAverageModel(BaseModel):\n",
    "    \n",
    "    def __init__(self, X_train, y_train, similarity='cosine'):\n",
    "        BaseModel.__init__(self, X_train, y_train)\n",
    "        \n",
    "        self._X_train['overall'] = self._y_train\n",
    "        self._user_product_matrix = pd.crosstab(self._X_train.reviewerID, self._X_train.asin, self._X_train.overall, aggfunc='max')\n",
    "        \n",
    "        if similarity == 'cosine': \n",
    "            cos_similarity = cosine_similarity(self._user_product_matrix.copy().fillna(0))\n",
    "            self._similarity = pd.DataFrame(cos_similarity, index=self._user_product_matrix.index)\n",
    "            self._similarity.columns = self._user_product_matrix.index\n",
    "        elif similarity == 'pearson':\n",
    "            pea_similarity = np.corrcoef(self._user_product_matrix.copy().fillna(0))\n",
    "            self._similarity = pd.DataFrame(pea_similarity, index=self._user_product_matrix.index)\n",
    "            self._similarity.columns = self._user_product_matrix.index\n",
    "        else:\n",
    "            raise Exception\n",
    "    \n",
    "    def predict(self, X):\n",
    "        results = np.array([])\n",
    "        for i, row in X.iterrows():\n",
    "            results = np.append(results, self._predict(row))\n",
    "        return results\n",
    "            \n",
    "    \n",
    "    def _predict(self, X):\n",
    "        if X['asin'] in self._user_product_matrix and X['reviewerID'] in self._user_product_matrix.index:\n",
    "            sim_scores = self._similarity[X['reviewerID']] \n",
    "            ratings_scores = self._user_product_matrix[X['asin']] \n",
    "\n",
    "            index_not_rated = ratings_scores[ratings_scores.isnull()].index\n",
    "            ratings_scores = ratings_scores.dropna()\n",
    "            sim_scores = sim_scores.drop(index_not_rated)\n",
    "\n",
    "            if sim_scores.sum() != 0:\n",
    "                return np.dot(ratings_scores, sim_scores)/sim_scores.sum()\n",
    "        \n",
    "        return 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3fca0ec5-ee80-4dfa-85ec-73b848e70395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollabKnnModel(BaseModel):\n",
    "    \n",
    "    def __init__(self, X_train, y_train, similarity='cosine', k=10):\n",
    "        BaseModel.__init__(self, X_train, y_train)\n",
    "        \n",
    "        self._X_train['overall'] = y_train\n",
    "        self._user_product_matrix = pd.crosstab(self._X_train.reviewerID, self._X_train.asin, self._X_train.overall, aggfunc='max')\n",
    "        \n",
    "        self._k = k        \n",
    "        \n",
    "        if similarity == 'cosine': \n",
    "            cos_similarity = cosine_similarity(self._user_product_matrix.copy().fillna(0))\n",
    "            self._similarity = pd.DataFrame(cos_similarity, index=self._user_product_matrix.index)\n",
    "            self._similarity.columns = self._user_product_matrix.index\n",
    "        elif similarity == 'pearson':\n",
    "            pea_similarity = np.corrcoef(self._user_product_matrix.copy().fillna(0))\n",
    "            self._similarity = pd.DataFrame(pea_similarity, index=self._user_product_matrix.index)\n",
    "            self._similarity.columns = self._user_product_matrix.index\n",
    "        else:\n",
    "            raise Exception\n",
    "            \n",
    "    def predict(self, X):\n",
    "        results = np.array([])\n",
    "        for i, row in X.iterrows():\n",
    "            results = np.append(results, self._predict(row))\n",
    "        return results\n",
    "    \n",
    "    def _knn_filtered(self, user_id, product_id, k):\n",
    "        return self._similarity[user_id][~np.isnan(self._user_product_matrix[product_id])].sort_values(ascending=False).head(k)\n",
    "    \n",
    "    def _predict(self, X):\n",
    "        if X['asin'] in self._user_product_matrix and X['reviewerID'] in self._user_product_matrix.index:\n",
    "            neighbours = self._knn_filtered(X['reviewerID'], X['asin'], self._k)\n",
    "\n",
    "            if not len(neighbours):\n",
    "                return 2.5\n",
    "            \n",
    "            ratings = self._user_product_matrix[X['asin']][neighbours.index.values].to_numpy().astype(float)\n",
    "            weights = neighbours.values.astype(float)\n",
    "            \n",
    "            if weights.sum() != 0:\n",
    "                return np.dot(ratings, weights)/weights.sum()\n",
    "        return 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4fb29-2dfc-4694-8cb9-3e8cae335c9f",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2a85d90b-a358-4037-9981-cf48a0c79a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis:\n",
    "    \n",
    "    def __init__(self, X_train, y_train):\n",
    "        self._pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SVC())])\n",
    "\n",
    "        self._pipeline.fit(X_train['sentence'], y_train)\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self._pipeline.predict(X['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d1a8bc-553b-4c7a-858e-1e1d0e6b31f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Collab Filtering With Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f3cb5cf9-c094-4d00-8d5a-fd2716ef095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedAverageModelSentimentAnalysis(WeightedAverageModel):\n",
    "    \n",
    "    def __init__(self, sa_model, X_train, y_train, similarity='cosine'):\n",
    "        review_text_ratings = sa_model.predict(X_train)\n",
    "        y_train = review_text_ratings\n",
    "        \n",
    "        WeightedAverageModel.__init__(self, X_train, y_train, similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16142b2a-6815-4897-af32-897d8d536e31",
   "metadata": {},
   "source": [
    "## Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a52644c4-bda5-448a-9f24-33bce4b30195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid:\n",
    "    \n",
    "    def __init__(self, content, collab, sentiment):\n",
    "        self._content = content\n",
    "        self._collab = collab\n",
    "        self._sentiment = sentiment\n",
    "        \n",
    "    def predict(self, X):\n",
    "        pred_content = self._content.predict(X)\n",
    "        pred_collab = self._collab.predict(X)\n",
    "        pred_sentiment = self._sentiment.predict(X)\n",
    "        return (pred_content + pred_collab + pred_sentiment)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1422d9-9495-4ec7-b061-01a3bb22a252",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8efa39da-f9e4-4424-bc29-64179e81da82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5201, 6)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_rm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c8c3c3ea-389f-4a25-b2ad-4f07db40576c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4680, 6)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4679bba1-7798-42d0-9244-e04589875f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(521, 6)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8446061d-7ffd-4146-b036-2640a6943ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = BaseModel(X_rm, y_rm)\n",
    "content_model = ContentBasedModel(X_rm, y_rm, meta_df)\n",
    "collab_avg_model = CollabWeightedAverageModel(X_rm, y_rm)\n",
    "collab_knn_model = CollabKnnModel(X_rm, y_rm, 'pearson', 10)\n",
    "sa_model = SentimentAnalysis(X_sa, y_sa)\n",
    "collab_sa_model = WeightedAverageModelSentimentAnalysis(sa_model, X_rm, y_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "178a6ed2-afc4-4d1c-9fef-cdd419a2b19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModel\n",
      "RMSE: 2.1121393035545646\n",
      "MAE: 1.9779270633397312\n",
      "\n",
      "ContentBasedModel\n",
      "RMSE: 1.3281248235858527\n",
      "MAE: 0.7159309021113244\n",
      "\n",
      "CollabWeightedAverageModel\n",
      "RMSE: 1.4475891672599095\n",
      "MAE: 1.0925526076627794\n",
      "\n",
      "CollabKnnModel\n",
      "RMSE: 1.247533267076578\n",
      "MAE: 0.8652152379490317\n",
      "\n",
      "WeightedAverageModelSentimentAnalysis\n",
      "RMSE: 1.5157525652053132\n",
      "MAE: 1.0954727692929918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(base_model, X_test, y_test)\n",
    "print_score(content_model, X_test, y_test)\n",
    "print_score(collab_avg_model, X_test, y_test)\n",
    "print_score(collab_knn_model, X_test, y_test)\n",
    "print_score(collab_sa_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d375a11a-c2b8-40a8-b3d3-b5f78ce4c152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid\n",
      "RMSE: 1.1901285672499156\n",
      "MAE: 0.8389022014321332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hybrid_model = Hybrid(content_model, collab_knn_model, collab_sa_model)\n",
    "print_score(hybrid_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cd30979b-36ba-4ac1-8db9-1e8abc507284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User AE06RDYJF5SKY has previously enjoyed:\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "\n",
      "We now recommend him:\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "<span class=\"a-size-medium a-color-secondary\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_k_recommendations(hybrid_model, 'AE06RDYJF5SKY', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438e16d-9164-4fb5-a182-5cd01b644b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
