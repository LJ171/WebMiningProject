{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc1f021c-5d36-4963-a41d-c7f86d43929a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007eff28-d1f2-4b84-9d71-37bf1df892f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, brown\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de667a9b-839b-4a34-9e89-77a15630fbee",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4f7ce-123b-4216-9f99-75696931b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_name = 'CDs_and_Vinyl'\n",
    "min_amount_product_mentions = 20\n",
    "min_amount_user_mentions = 20\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "MIN_WORDS = 4\n",
    "MAX_WORDS = 200\n",
    "\n",
    "PATTERN_S = re.compile(\"\\'s\")  # matches `'s` from text  \n",
    "PATTERN_RN = re.compile(\"\\\\r\\\\n\") #matches `\\r` and `\\n`\n",
    "PATTERN_PUNC = re.compile(r\"[^\\w\\s]\") # matches all non 0-9 A-z whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94841b58-0dbd-401e-ae36-14e9316ade36",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403df00a-550e-4fb8-a01e-4b42050aa9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_metadata(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer() \n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Series of cleaning. String to lower case, remove non words characters and numbers.\n",
    "        text (str): input text\n",
    "    return (str): modified initial text\n",
    "    \"\"\"\n",
    "    text = text.lower()  # lowercase text\n",
    "    text = re.sub(PATTERN_S, ' ', text)\n",
    "    text = re.sub(PATTERN_RN, ' ', text)\n",
    "    text = re.sub(PATTERN_PUNC, ' ', text)\n",
    "    return text\n",
    "\n",
    "def tokenizer(sentence, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS, lemmatize=True):\n",
    "    \"\"\"\n",
    "    Lemmatize, tokenize, crop and remove stop words.\n",
    "    \"\"\"\n",
    "    if lemmatize:\n",
    "        stemmer = WordNetLemmatizer()\n",
    "        tokens = [stemmer.lemmatize(w) for w in nltk.word_tokenize(sentence)]\n",
    "    else:\n",
    "        tokens = [w for w in word_tokenize(sentence)]\n",
    "    token = [w for w in tokens if (len(w) > min_words and len(w) < max_words\n",
    "                                                        and w not in stopwords)]\n",
    "    return tokens    \n",
    "\n",
    "\n",
    "def clean_sentences(df):\n",
    "    \"\"\"\n",
    "    Remove irrelavant characters (in new column clean_sentence).\n",
    "    Lemmatize, tokenize words into list of words (in new column tok_lem_sentence).\n",
    "    \"\"\"\n",
    "    df['clean_sentence'] = df['sentence'].apply(clean_text)\n",
    "    df['tok_lem_sentence'] = df['clean_sentence'].apply(\n",
    "        lambda x: tokenizer(x, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS, lemmatize=True))\n",
    "    return df\n",
    "\n",
    "def get_data(subset_name, min_amount_product_mentions, min_amount_user_mentions, test_size=0.1, random_state=42):    \n",
    "    if os.path.exists(os.path.join('data', f'{subset_name}_preprocessed.txt')):\n",
    "        print('Loading preprocessed rating data...')\n",
    "        rating_df = pd.read_csv(os.path.join('data', f'{subset_name}_preprocessed.txt'))\n",
    "    else:\n",
    "        print('Loading rating data...')\n",
    "        data = []\n",
    "        with gzip.open(os.path.join('data', f'{subset_name}.json.gz')) as f:\n",
    "            for l in f:\n",
    "                data.append(json.loads(l.strip()))\n",
    "        rating_df = pd.DataFrame.from_dict(data)\n",
    "        \n",
    "        print('Preprocessing rating_df')\n",
    "        rating_df = rating_df[['asin', 'reviewerID', 'overall', 'reviewText']]\n",
    "        rating_df = rating_df.drop_duplicates()    \n",
    "        rating_df = rating_df[rating_df['asin'].map(rating_df['asin'].value_counts()) >= min_amount_product_mentions]\n",
    "        rating_df = rating_df[rating_df['reviewerID'].map(rating_df['reviewerID'].value_counts()) >= min_amount_user_mentions]\n",
    "        rating_df = rating_df[~rating_df.reviewText.isna()]\n",
    "        rating_df.rename(columns={'reviewText':'sentence'}, inplace=True)\n",
    "                \n",
    "        print('Cleaning sentences...')\n",
    "        rating_df = clean_sentences(rating_df)\n",
    "        \n",
    "        print('Saving rating_df...')\n",
    "        rating_df.to_csv(os.path.join('data', f'{subset_name}_preprocessed.txt'))\n",
    "    \n",
    "    if os.path.exists(os.path.join('data', f'meta_{subset_name}_preprocessed.txt')):\n",
    "        print('Loading preprocessed meta data...')\n",
    "        meta_df = pd.read_csv(os.path.join('data', f'meta_{subset_name}_preprocessed.txt'))\n",
    "    else:\n",
    "        print('Loading meta data...')\n",
    "        data = []\n",
    "        with gzip.open(os.path.join('data', f'meta_{subset_name}.json.gz')) as f:\n",
    "            for l in f:\n",
    "                data.append(json.loads(l.strip()))\n",
    "        meta_df = pd.DataFrame.from_dict(data)\n",
    "        \n",
    "        print('Preprocessing rating_df')\n",
    "        meta_df = meta_df[meta_df['asin'].isin(rating_df['asin'].unique())]        \n",
    "        meta_df['category'] = meta_df['category'].apply(lambda x: ','.join(map(str, x)))\n",
    "        meta_df['description'] = meta_df['description'].apply(lambda x: ','.join(map(str, x)))\n",
    "        meta_df['feature'] = meta_df['feature'].apply(lambda x: ','.join(map(str, x)))\n",
    "        meta_df['metadata'] = meta_df['category'] + ' ' + meta_df['description'] + ' ' + meta_df['title'] + ' ' + meta_df['feature']\n",
    "\n",
    "        print('Cleaning metadata...')\n",
    "        meta_df['metadata'] = meta_df['metadata'].map(lambda x:preprocess_metadata(x))\n",
    "               \n",
    "        print('Saving meta_df...')\n",
    "        meta_df.to_csv(os.path.join('data', f'meta_{subset_name}_preprocessed.txt'))   \n",
    "    \n",
    "    rating_df = pd.merge(rating_df, meta_df[['asin', 'metadata']], on='asin')\n",
    "    \n",
    "    # fix class distribution\n",
    "    class_size = rating_df[['asin', 'overall']].groupby('overall').count()['asin'].min()\n",
    "    \n",
    "    indices = np.array([])\n",
    "    for i in range(1,6):\n",
    "        indices = np.append(indices, np.random.choice(rating_df[rating_df['overall'] == i].index, class_size, replace=False))\n",
    "    \n",
    "    final = rating_df.copy()\n",
    "    final = final.iloc[indices.astype(int)]\n",
    "    \n",
    "    # split rating df in half -> half is used for sentiment analysis and the other half for the recommender models\n",
    "    X = final.drop(['overall'], axis=1)\n",
    "    y = final['overall']\n",
    "    \n",
    "    X_rm, X_sa, y_rm, y_sa = train_test_split(X, y, test_size=0.5, stratify=y, random_state=random_state)\n",
    "        \n",
    "    # split data of the sa half for the test set\n",
    "    X_sa, X_test, y_sa, y_test = train_test_split(X_sa, y_sa, test_size=test_size, stratify=y_sa, random_state=random_state)\n",
    "                        \n",
    "    return rating_df, final, X_rm, X_sa, X_test, y_rm, y_sa, y_test, meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62898617-48e6-410e-943d-4ef666791457",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df, final, X_rm, X_sa, X_test, y_rm, y_sa, y_test, meta_df = get_data(subset_name, min_amount_product_mentions, min_amount_user_mentions)\n",
    "del rating_df # clear up some space\n",
    "rating_df = final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ccc0f6-209c-463a-bc01-26c5567ca2d0",
   "metadata": {},
   "source": [
    "# Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6062ebb3-7d82-47b9-a620-0eeb3e08fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    print(model.__class__.__name__)\n",
    "    print(f'RMSE: {np.sqrt(mean_squared_error(y, y_pred))}')\n",
    "    print(f'MAE: {mean_absolute_error(y, y_pred)}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc02f6f-ccd5-4fd9-aebf-b3036ace2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_product_ids():\n",
    "    return rating_df['asin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04e5ba-3a74-484e-b16d-addc74fdb95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_recommendations(model, user_id, k=5):    \n",
    "    # only recommend items not reviewed by the user before\n",
    "    not_rated_product_ids = [x for x in get_all_product_ids() if x not in set(rating_df[rating_df['reviewerID'] == 'AEJAGHLC675A7']['asin'].values)]\n",
    "        \n",
    "    user_product_pairs = pd.DataFrame({\n",
    "        'asin': not_rated_product_ids,\n",
    "        'reviewerID': [user_id]*len(not_rated_product_ids)\n",
    "    })\n",
    "    user_product_pairs = pd.merge(user_product_pairs, meta_df[['asin', 'metadata']], on='asin')\n",
    "    user_product_pairs['predicted_rating'] = model.predict(user_product_pairs)\n",
    "    return user_product_pairs.sort_values(by='predicted_rating', ascending=False).head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f33c1-ad70-484e-8c82-76c52459ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_k_recommendations(model, user_id, k=5):\n",
    "    print(f'User {user_id} has previously enjoyed:')\n",
    "    previously_rated = rating_df[rating_df['reviewerID']==user_id].sort_values(by='overall',ascending=False).head(k)['asin'].values\n",
    "    print_product_titles(previously_rated)\n",
    "    print('')\n",
    "    print('We now recommend him:')\n",
    "    recommendations =  get_k_recommendations(model, user_id, k)['asin'].values\n",
    "    print_product_titles(recommendations)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a3119b-5003-49dd-817d-e8ffa61250c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_product_titles(ids):\n",
    "    titles = meta_df[meta_df['asin'].isin(ids)]['title'].values\n",
    "    for title in titles:\n",
    "        print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564f94a-82ca-4579-9f57-2211fcc012b5",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1725a8b-f7c5-4cab-a3d3-195d403548b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54fbf9-cc27-474b-810f-0eda53579d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    \n",
    "    def __init__(self, X_train, y_train):\n",
    "        self._X_train = X_train.copy()\n",
    "        self._y_train = y_train.copy()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return [2.5] * len(X)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c56a3-52f5-4094-855f-2e0801334635",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Content-based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c51ca-8699-4a24-b14c-974a3f256fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBasedSVMModel(BaseModel):\n",
    "    \n",
    "    def __init__(self, X_train, y_train, meta_df):\n",
    "        self._pipeline = Pipeline([('Vectorizer', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('SVM classifier', SVC())])\n",
    "\n",
    "        self._pipeline.fit(X_train['metadata'], y_train)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self._pipeline.predict(X['metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f4aa7b-d291-4720-9882-37cd780b1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBasedWeightedAverageModel(BaseModel):\n",
    "    \n",
    "    def __init__(self, X_train, y_train, meta_df):\n",
    "        BaseModel.__init__(self, X_train, y_train)\n",
    "        \n",
    "        self._X_train['overall'] = self._y_train\n",
    "        self._user_product_matrix = pd.crosstab(self._X_train.reviewerID, self._X_train.asin, self._X_train.overall, aggfunc='max')\n",
    "        \n",
    "        tfidf = TfidfVectorizer(smooth_idf = False, sublinear_tf = True)\n",
    "        tfidf_matrix = tfidf.fit_transform(meta_df['metadata'])\n",
    "        content_correlation = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "        self._similarity = pd.DataFrame(content_correlation, index=meta_df.asin.tolist(), columns=meta_df.asin.tolist())\n",
    "        self._similarity = self._similarity.drop_duplicates()\n",
    "        self._similarity = self._similarity.loc[:,~self._similarity.columns.duplicated()]\n",
    "        \n",
    "    def predict(self, X):\n",
    "        results = np.array([])\n",
    "        for i, row in X.iterrows():\n",
    "            results = np.append(results, self._predict(row))\n",
    "        return results\n",
    "            \n",
    "    \n",
    "    def _predict(self, X):\n",
    "        if X['asin'] in self._similarity and X['asin'] in self._similarity.index and X['reviewerID'] in self._user_product_matrix.index:\n",
    "            ratings_scores = self._user_product_matrix.loc[X['reviewerID']] \n",
    "            ratings_scores = ratings_scores.dropna()\n",
    "            \n",
    "            if len(ratings_scores.index.values) > 0:\n",
    "                sim_scores = self._similarity[X['asin']]\n",
    "                if all(i in sim_scores.index.values for i in ratings_scores.index.values):\n",
    "                    sim_scores = sim_scores.loc[ratings_scores.index.values]\n",
    "                    if sim_scores.sum() != 0:\n",
    "                        return np.dot(ratings_scores, sim_scores)/sim_scores.sum()\n",
    "        \n",
    "        return 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb8b07e-fd96-4ef7-b24e-75ed09571e93",
   "metadata": {},
   "source": [
    "## User-based Collab Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b9ede6-5a82-404b-a344-ac0d5831718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollabWeightedAverageModel(BaseModel):\n",
    "    \n",
    "    def __init__(self, X_train, y_train, similarity='cosine'):\n",
    "        BaseModel.__init__(self, X_train, y_train)\n",
    "        \n",
    "        self._X_train['overall'] = self._y_train\n",
    "        self._user_product_matrix = pd.crosstab(self._X_train.reviewerID, self._X_train.asin, self._X_train.overall, aggfunc='max')\n",
    "        \n",
    "        if similarity == 'cosine': \n",
    "            cos_similarity = cosine_similarity(self._user_product_matrix.copy().fillna(0))\n",
    "            self._similarity = pd.DataFrame(cos_similarity, index=self._user_product_matrix.index)\n",
    "            self._similarity.columns = self._user_product_matrix.index\n",
    "        elif similarity == 'pearson':\n",
    "            pea_similarity = np.corrcoef(self._user_product_matrix.copy().fillna(0))\n",
    "            self._similarity = pd.DataFrame(pea_similarity, index=self._user_product_matrix.index)\n",
    "            self._similarity.columns = self._user_product_matrix.index\n",
    "        else:\n",
    "            raise Exception\n",
    "    \n",
    "    def predict(self, X):\n",
    "        results = np.array([])\n",
    "        for i, row in X.iterrows():\n",
    "            results = np.append(results, self._predict(row))\n",
    "        return results\n",
    "            \n",
    "    \n",
    "    def _predict(self, X):\n",
    "        if X['asin'] in self._user_product_matrix and X['reviewerID'] in self._user_product_matrix.index:\n",
    "            sim_scores = self._similarity[X['reviewerID']] \n",
    "            ratings_scores = self._user_product_matrix[X['asin']] \n",
    "\n",
    "            index_not_rated = ratings_scores[ratings_scores.isnull()].index\n",
    "            ratings_scores = ratings_scores.dropna()\n",
    "            sim_scores = sim_scores.drop(index_not_rated)\n",
    "\n",
    "            if sim_scores.sum() != 0:\n",
    "                return np.dot(ratings_scores, sim_scores)/sim_scores.sum()\n",
    "        \n",
    "        return 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca0ec5-ee80-4dfa-85ec-73b848e70395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollabKnnModel(BaseModel):\n",
    "    \n",
    "    def __init__(self, X_train, y_train, similarity='cosine', k=10):\n",
    "        BaseModel.__init__(self, X_train, y_train)\n",
    "        \n",
    "        self._X_train['overall'] = y_train\n",
    "        self._user_product_matrix = pd.crosstab(self._X_train.reviewerID, self._X_train.asin, self._X_train.overall, aggfunc='max')\n",
    "        \n",
    "        self._k = k        \n",
    "        \n",
    "        if similarity == 'cosine': \n",
    "            cos_similarity = cosine_similarity(self._user_product_matrix.copy().fillna(0))\n",
    "            self._similarity = pd.DataFrame(cos_similarity, index=self._user_product_matrix.index)\n",
    "            self._similarity.columns = self._user_product_matrix.index\n",
    "        elif similarity == 'pearson':\n",
    "            pea_similarity = np.corrcoef(self._user_product_matrix.copy().fillna(0))\n",
    "            self._similarity = pd.DataFrame(pea_similarity, index=self._user_product_matrix.index)\n",
    "            self._similarity.columns = self._user_product_matrix.index\n",
    "        else:\n",
    "            raise Exception\n",
    "            \n",
    "    def predict(self, X):\n",
    "        results = np.array([])\n",
    "        for i, row in X.iterrows():\n",
    "            results = np.append(results, self._predict(row))\n",
    "        return results\n",
    "    \n",
    "    def _knn_filtered(self, user_id, product_id, k):\n",
    "        return self._similarity[user_id][~np.isnan(self._user_product_matrix[product_id])].sort_values(ascending=False).head(k)\n",
    "    \n",
    "    def _predict(self, X):\n",
    "        if X['asin'] in self._user_product_matrix and X['reviewerID'] in self._user_product_matrix.index:\n",
    "            neighbours = self._knn_filtered(X['reviewerID'], X['asin'], self._k)\n",
    "\n",
    "            if not len(neighbours):\n",
    "                return 2.5\n",
    "            \n",
    "            ratings = self._user_product_matrix[X['asin']][neighbours.index.values].to_numpy().astype(float)\n",
    "            weights = neighbours.values.astype(float)\n",
    "            \n",
    "            if weights.sum() != 0:\n",
    "                return np.dot(ratings, weights)/weights.sum()\n",
    "        return 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4fb29-2dfc-4694-8cb9-3e8cae335c9f",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a85d90b-a358-4037-9981-cf48a0c79a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis:\n",
    "    \n",
    "    def __init__(self, X_train, y_train):\n",
    "        self._pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SVC())])\n",
    "\n",
    "        self._pipeline.fit(X_train['sentence'], y_train)\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self._pipeline.predict(X['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d1a8bc-553b-4c7a-858e-1e1d0e6b31f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Collab Filtering With Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb5cf9-c094-4d00-8d5a-fd2716ef095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedAverageModelSentimentAnalysis(CollabWeightedAverageModel):\n",
    "    \n",
    "    def __init__(self, sa_model, X_train, y_train, similarity='cosine'):\n",
    "        review_text_ratings = sa_model.predict(X_train)\n",
    "        y_train = review_text_ratings\n",
    "        \n",
    "        CollabWeightedAverageModel.__init__(self, X_train, y_train, similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16142b2a-6815-4897-af32-897d8d536e31",
   "metadata": {},
   "source": [
    "## Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52644c4-bda5-448a-9f24-33bce4b30195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid:\n",
    "    \n",
    "    def __init__(self, content, collab, sentiment):\n",
    "        self._content = content\n",
    "        self._collab = collab\n",
    "        self._sentiment = sentiment\n",
    "        \n",
    "    def predict(self, X):\n",
    "        pred_content = self._content.predict(X)\n",
    "        pred_collab = self._collab.predict(X)\n",
    "        pred_sentiment = self._sentiment.predict(X)\n",
    "        return (pred_content + pred_collab + pred_sentiment)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1422d9-9495-4ec7-b061-01a3bb22a252",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa39da-f9e4-4424-bc29-64179e81da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c3c3ea-389f-4a25-b2ad-4f07db40576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679bba1-7798-42d0-9244-e04589875f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ff4786-3b44-452a-a371-f38206063281",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = BaseModel(X_rm, y_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd7056-61fb-4bdd-92f1-5c16e32e5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539fb8d-b1c1-4928-a481-e7f4842b033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_svm_model = ContentBasedSVMModel(X_rm, y_rm, meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8798d3c-f53d-40be-b775-1f8f3ce8ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(content_svm_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8099ca7-fe8d-420f-b94e-805eae64339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_avg_model = ContentBasedWeightedAverageModel(X_rm, y_rm, meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42db821-6a36-4050-9998-c2f9aa1f543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(content_avg_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f797a78e-c5c9-415a-bd5f-735e9a821c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "collab_avg_model = CollabWeightedAverageModel(X_rm, y_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588a71c0-fa9a-4d96-b33f-4bec56610f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(collab_avg_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2145b6-da93-4eb3-bee5-44bef9728b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collab_knn_model = CollabKnnModel(X_rm, y_rm, 'pearson', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd159d-2480-40c8-beed-324d70b67542",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(collab_knn_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396ed91-624a-495f-96ac-694f8fb0ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_model = SentimentAnalysis(X_sa, y_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92e167-5c15-4bf3-8dac-a8cf6c25581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(sa_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc3d29-275f-42de-a945-417a4c654cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "collab_sa_model = WeightedAverageModelSentimentAnalysis(sa_model, X_rm, y_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8446061d-7ffd-4146-b036-2640a6943ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(collab_sa_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed55ec8-cbfa-4596-962e-c84ca316f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model = Hybrid(content_svm_model, collab_knn_model, collab_sa_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a6ed2-afc4-4d1c-9fef-cdd419a2b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(hybrid_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30979b-36ba-4ac1-8db9-1e8abc507284",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_k_recommendations(hybrid_model, 'AE06RDYJF5SKY', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438e16d-9164-4fb5-a182-5cd01b644b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
